{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imagen](../../images/ejercicios.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 1 \n",
    "\n",
    "Muestra los nombres de los Trending Developers in Github.\n",
    "\n",
    "https://github.com/trending/developers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as req\n",
    "\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/trending/developers'\n",
    "\n",
    "html=req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bdring',\n",
       " 'YagizDegirmenci',\n",
       " 'NicoSchlömer',\n",
       " 'MatthiasFey',\n",
       " 'GregBergé',\n",
       " 'Juliette',\n",
       " 'AnthonyFu',\n",
       " 'JoviDeCroock',\n",
       " 'JanDeDobbeleer',\n",
       " 'FelixAngelov',\n",
       " 'RicoSuter',\n",
       " 'CharlesKerr',\n",
       " 'TanerŞener',\n",
       " 'NunoMaduro',\n",
       " 'Miladakarie',\n",
       " 'MichaelBui',\n",
       " 'TatsuroShibamura',\n",
       " 'NicolaCorti',\n",
       " 'DaviddelaIglesiaCastro',\n",
       " 'CarlosAlexandroBecker',\n",
       " 'SeanDoyle',\n",
       " 'MichaIng',\n",
       " 'Steven',\n",
       " 'lestrrat',\n",
       " 'JoakimSørensen']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def elimina(t):\n",
    "\n",
    "    t = t.replace(' ','')\n",
    "    t = t.replace('\\n\\n',' ')\n",
    "    res=t.strip().split()[0] \n",
    "    return res \n",
    "\n",
    "\n",
    "dev = [d.text for d in soup.find_all('h1', class_='h3 lh-condensed')]\n",
    "\n",
    "dev = list(map(elimina, dev))\n",
    "\n",
    "dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Imprime los repositorios de Github relacionadps con Python.\n",
    "\n",
    "https://github.com/trending/python?since=daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Janspiry/',\n",
       " 'aqlaboratory/',\n",
       " 'microsoft/',\n",
       " 'pengzhiliang/',\n",
       " 'rougier/',\n",
       " 'PaddlePaddle/',\n",
       " 'CyberPunkMetalHead/',\n",
       " 'PaddlePaddle/',\n",
       " 'frappe/',\n",
       " 'miloyip/',\n",
       " 'lucidrains/',\n",
       " 'kubernetes-client/',\n",
       " 'donnemartin/',\n",
       " 'JingyunLiang/',\n",
       " 'redis/',\n",
       " 'localstack/',\n",
       " 'UKPLab/',\n",
       " 'floodsung/',\n",
       " 'sivel/',\n",
       " 'PyCQA/',\n",
       " '0xCGonzalo/',\n",
       " 'pre-commit/',\n",
       " 'albumentations-team/',\n",
       " 'kovidgoyal/',\n",
       " 'awslabs/']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo = [r.text for r in soup.find_all('h1', class_='h3 lh-condensed')]\n",
    "\n",
    "repo = list(map(elimina, repo))\n",
    "\n",
    "repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "Muestra todos los links de imagenes de la página de wikipedis de Walt Disney.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Walt_Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8c/Extended-protection-shackle.svg/20px-Extended-protection-shackle.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Disneyland_Resort_logo.svg/135px-Disneyland_Resort_logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " 'upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " 'en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/footer/wikimedia-button.png',\n",
       " '/static/images/footer/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlaces = soup.find_all('img')\n",
    "\n",
    "images_src = [img['src'].replace('//', '') for img in soup.find_all('img')]\n",
    "\n",
    "images_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Crea una lista de todos los links de la página de wikipedia de Python.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute',\n",
       " 'https://stats.wikimedia.org/#/en.wikipedia.org',\n",
       " 'https://foundation.wikimedia.org/wiki/Cookie_statement',\n",
       " 'https://wikimediafoundation.org/',\n",
       " 'https://www.mediawiki.org/']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enlaces = [a.get('href') for a in soup.find_all('a')]\n",
    "\n",
    "enlaces[-5:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Crea una lista con los nombres de las 10 personas más buscadas por el FBI:\n",
    "\n",
    "https://www.fbi.gov/wanted/topten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'OCTAVIANO JUAREZ-CORRO',\n",
       " 'RAFAEL CARO-QUINTERO']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buscados = [f.text.strip().replace('\\n', '') for f in soup.find_all('h3', class_= 'title')]\n",
    "\n",
    "buscados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 6\n",
    "\n",
    "Crea un dataframe con los ultimos 20 terremotos desde el Centro Sismológico Euromediterráneo que incluya fecha, hora, latitud, longitud, profundidad, magnitud y region.\n",
    "\n",
    "https://www.emsc-csem.org/Earthquake/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>prof</th>\n",
       "      <th>mag</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>18:11:47.5</td>\n",
       "      <td>28.56 N</td>\n",
       "      <td>17.84 W</td>\n",
       "      <td>11</td>\n",
       "      <td>2.8</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>17:54:30.0</td>\n",
       "      <td>35.40 N</td>\n",
       "      <td>139.20 E</td>\n",
       "      <td>20</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NEAR S. COAST OF HONSHU, JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>17:53:15.0</td>\n",
       "      <td>6.18 N</td>\n",
       "      <td>126.69 E</td>\n",
       "      <td>70</td>\n",
       "      <td>4.6</td>\n",
       "      <td>MINDANAO, PHILIPPINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>17:37:32.6</td>\n",
       "      <td>28.57 N</td>\n",
       "      <td>17.83 W</td>\n",
       "      <td>10</td>\n",
       "      <td>2.8</td>\n",
       "      <td>CANARY ISLANDS, SPAIN REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-16</td>\n",
       "      <td>17:36:25.0</td>\n",
       "      <td>41.39 N</td>\n",
       "      <td>20.94 E</td>\n",
       "      <td>3</td>\n",
       "      <td>3.2</td>\n",
       "      <td>REPUBLIC OF NORTH MACEDONIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date        time   latitude   longitude prof  mag  \\\n",
       "0  2021-11-16  18:11:47.5  28.56 N     17.84 W     11  2.8   \n",
       "1  2021-11-16  17:54:30.0  35.40 N    139.20 E     20  4.4   \n",
       "2  2021-11-16  17:53:15.0   6.18 N    126.69 E     70  4.6   \n",
       "3  2021-11-16  17:37:32.6  28.57 N     17.83 W     10  2.8   \n",
       "4  2021-11-16  17:36:25.0  41.39 N     20.94 E      3  3.2   \n",
       "\n",
       "                            region  \n",
       "0     CANARY ISLANDS, SPAIN REGION  \n",
       "1   NEAR S. COAST OF HONSHU, JAPAN  \n",
       "2            MINDANAO, PHILIPPINES  \n",
       "3     CANARY ISLANDS, SPAIN REGION  \n",
       "4      REPUBLIC OF NORTH MACEDONIA  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "table = soup.find_all('tr', class_= 'normal')\n",
    "\n",
    "for td in table:\n",
    "    #print(td)\n",
    "    temp_fecha = td.contents[3].find('a').text\n",
    "    temp_fecha = temp_fecha.split()\n",
    "    \n",
    "    fecha = temp_fecha[0]\n",
    "    hora = temp_fecha[1]\n",
    "    \n",
    "    latitud = td.contents[4].text + td.contents[5].text\n",
    "    longitud = td.contents[6].text + td.contents[7].text\n",
    "    \n",
    "    prof=td.contents[8].text\n",
    "    \n",
    "    magn=td.contents[10].text\n",
    "    \n",
    "    region = td.contents[11].text\n",
    "    \n",
    "    data.append([fecha, hora, latitud, longitud, prof, magn, region])\n",
    "\n",
    "terremotos = pd.DataFrame(data, columns = ['date', 'time', 'latitude', 'longitude','prof', 'mag', 'region'])\n",
    "terremotos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 7\n",
    "\n",
    "Crea un diccionario cuyas keys sean los idiomas y los values sean los articulos para cada uno de la pagina de wikipedia.\n",
    "\n",
    "https://www.wikipedia.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English': '6383000',\n",
       " 'æ\\x97¥æ\\x9c¬èª\\x9e': '1292000',\n",
       " 'ÐÑ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹': '1756000',\n",
       " 'Deutsch': '2617000',\n",
       " 'EspaÃ±ol': '1717000',\n",
       " 'FranÃ§ais': '2362000',\n",
       " 'ä¸\\xadæ\\x96\\x87': '1231000',\n",
       " 'Italiano': '1718000',\n",
       " 'PortuguÃªs': '1074000',\n",
       " 'Polski': '1490000'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contenido = soup.find_all('div', class_= 'central-featured')\n",
    "\n",
    "idioma = contenido[0].find_all('strong')\n",
    "idioma = [e.text.replace('\\xa0', '') for e in idioma]\n",
    "\n",
    "articulos = contenido[0].find_all('bdi')\n",
    "articulos = [e.text.replace('\\xa0', '').replace('+', '') for e in articulos]\n",
    "\n",
    "\n",
    "{k:v for k,v in zip(idioma, articulos)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 8\n",
    "\n",
    "Crea una lista de las clases de datos disponibles en:\n",
    "\n",
    "https://data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport',\n",
       " 'Digital service performance',\n",
       " 'Government reference data']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_tipo = [tipo.text for tipo in soup.find_all('h3')]\n",
    "\n",
    "datos_tipo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 9\n",
    "\n",
    "Crea una lista con los 10 idiomas más hablados.\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = req.get(url).text\n",
    "\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mandarin',\n",
       " 'Spanish',\n",
       " 'English',\n",
       " 'Hindi',\n",
       " 'Arabic',\n",
       " 'Portuguese',\n",
       " 'Bengali',\n",
       " 'Russian',\n",
       " 'Japanese',\n",
       " 'Punjabi']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "table = soup.find_all('table', class_= 'wikitable')\n",
    "\n",
    "table_idi = [idioma.text for idioma in table[1].find_all('a') if len(idioma.text) > 3]\n",
    "    \n",
    "table_idi[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio 10\n",
    "\n",
    "Crea una dataframe de las 250 peliculas top en IMDB con nombre, año, director y estrellas.\n",
    "\n",
    "https://www.imdb.com/chart/top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_pelis_1 = []\n",
    "lista_pelis_2 = []\n",
    "\n",
    "html = req.get(url).text\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "pelis = soup.find_all('td', class_='titleColumn')\n",
    "pelis1 = soup.find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpia_txt(t):\n",
    "    t = re.sub('[^0-9]+', '', t)\n",
    "    return t\n",
    "\n",
    "def limpia_ratio(t):\n",
    "    t = re.sub('([^0-9])', '', t)\n",
    "    return t[:1]+'.'+t[1:2] \n",
    "\n",
    "res = []\n",
    "res_rate = []\n",
    "for td in pelis1:\n",
    "    anyo = td.find('span', class_='secondaryInfo')\n",
    "    res.append(str(anyo))\n",
    "    rate = td.find('strong')\n",
    "    res_rate.append(str(rate))\n",
    "\n",
    "\n",
    "\n",
    "fechas = list(map(limpia_txt, res))\n",
    "ratios = list(map(limpia_ratio, res_rate))\n",
    "\n",
    "df_fecha = pd.DataFrame(fechas[1:], columns = ['Initial release'])\n",
    "df_ratio = pd.DataFrame(ratios[1:], columns = ['Stars'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Director</th>\n",
       "      <th>Initial release</th>\n",
       "      <th>Starts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cadena perpetua</td>\n",
       "      <td>Frank Darabont (dir.)</td>\n",
       "      <td>1994</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>1972</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>Francis Ford Coppola (dir.)</td>\n",
       "      <td>1974</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>Christopher Nolan (dir.)</td>\n",
       "      <td>2008</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>Sidney Lumet (dir.)</td>\n",
       "      <td>1957</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Movie                     Director Initial release Starts\n",
       "0        Cadena perpetua        Frank Darabont (dir.)            1994    9.2\n",
       "1             El padrino  Francis Ford Coppola (dir.)            1972    9.1\n",
       "2   El padrino: Parte II  Francis Ford Coppola (dir.)            1974    9.0\n",
       "3    El caballero oscuro     Christopher Nolan (dir.)            2008    9.0\n",
       "4  12 hombres sin piedad          Sidney Lumet (dir.)            1957    8.9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in pelis:\n",
    "    \n",
    "    anyo = p.find('secondaryInfo')\n",
    "    \n",
    "    temp_tit = p.find('a')\n",
    "    titulo = temp_tit.text\n",
    "\n",
    "    if 'title' in temp_tit.attrs:\n",
    "        director = temp_tit['title'].split(',')[0]\n",
    "\n",
    "    lista_pelis_1.append([titulo, director])\n",
    "    \n",
    "lista_pelis_1\n",
    "\n",
    "\n",
    "peliculas = pd.DataFrame(lista_pelis_1, columns = ['Movie','Director'])\n",
    "peliculas['Initial release'] = df_fecha\n",
    "peliculas['Starts'] = df_ratio\n",
    "\n",
    "peliculas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
